{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOk+NB6g8J+rEhGBalshjU0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmalicekim/NLP_practice_mine/blob/master/14.%20BERT%EC%9D%98%20%EC%9D%B4%ED%95%B4%EC%99%80%20%EA%B0%84%EB%8B%A8%ED%95%9C%20%ED%99%9C%EC%9A%A9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_pPhnqsJ59f",
        "outputId": "8b02a950-3a57-4c1f-b036-b77573d2ca2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.24.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "clf = pipeline('sentiment-analysis')\n",
        "result = clf('what a beautiful day!')[0]\n",
        "print('감성분석 결과: %s, 감성스코어: %.4f'%(result['label'], result['score']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_4OXBqjKMgy",
        "outputId": "d0e31d19-44da-4ef9-ec7d-0473b581fce3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "감성분석 결과: POSITIVE, 감성스코어: 0.9999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf = pipeline('sentiment-analysis')\n",
        "result = clf('is it right?')\n",
        "print(result)\n",
        "# print('감성분석 결과: %s, 감성스코어: %.4f'%(result['label'], result['score']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdDhAM1XKyR2",
        "outputId": "6bacef29-cda6-4ce2-8631-1ad2152efdbf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'POSITIVE', 'score': 0.9992733597755432}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 문서 생성 예시\n",
        "\n",
        "# 사용은 쉬우나, 이런 식의 사용은 제약이 많음. \n",
        "# e.g. BERT는 기본적으로 512개의 토큰을 사용하나, 문장을 토큰화한 결과가 이 숫자를 넘게 되면 바로 에러 발생\n",
        "# 그렇게 되면 더 많은 지식과 다른 사용법이 필요함\n",
        "\n",
        "text_generator = pipeline('text-generation')\n",
        "result = text_generator('Alice was beginning to get very tired of sitting by her sister on the bank,')\n",
        "print(result)\n",
        "print(result[0]['generated_text'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72F3F9aCLIj2",
        "outputId": "2c657798-2a18-48d5-c8f1-4ecd12690ce3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to gpt2 and revision 6c0e608 (https://huggingface.co/gpt2).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py:1364: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 50 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': \"Alice was beginning to get very tired of sitting by her sister on the bank, and she knew she was not being paid for her work in the country, or at least not on her own account. There didn't seem to be anyone who knew how\"}]\n",
            "Alice was beginning to get very tired of sitting by her sister on the bank, and she knew she was not being paid for her work in the country, or at least not on her own account. There didn't seem to be anyone who knew how\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 14.5 자동 클래스를 이용한 토크나이저와 모형의 사용"
      ],
      "metadata": {
        "id": "2H9UW48tel3r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mrpc (The Microsoft Research Paraphrase Corpus): 의미적으로 유사한 문장의 페어와 그렇지 않은 문장의 페어로 구성하여 두 문장의 의미적 유사성을 학습할 수 있도록 만든 데이터셋 \n",
        "#'bert-base-cased-finetuned-mrpc : 사전학습된 모형으로 다시 MRPC 데이터셋에 대해 미세조정학습을 진행함\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification \n",
        "import torch\n",
        "\n",
        "# Auto Classes를 이용해 사전학습된 내용에 맞는 토크나이저와 모형을 자동으로 설정\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased-finetuned-mrpc') \n",
        "model = AutoModelForSequenceClassification.from_pretrained('bert-base-cased-finetuned-mrpc') \n",
        "\n",
        "# 의미적으로 유사한 두 문장을 선언\n",
        "input_sentence = \"She angered me with her inappropriate comments, rumor-spreading, and discrespectfulness at the formal dinner table\"\n",
        "target_sequence = 'She made me angry when she was rude at dinner'\n",
        "\n",
        "\n",
        "# 토큰화\n",
        "tokens = tokenizer(input_sentence, target_sequence, return_tensors='pt') \n",
        "\n",
        "# 모형으로 결과 예측 \n",
        "# logits : the raw(non-normalized) scores for classification model before Softmax\n",
        "logits = model(**tokens).logits \n",
        "\n",
        "# 소프트맥스를 이용하여 결과값을 클래스에 대한 확률로 변환 \n",
        "results = torch.softmax(logits, dim=1).tolist()[0] \n",
        "\n",
        "for i, label in enumerate(['no', 'yes']):\n",
        "  print(f\"{label}: {int(round(results[i]*100))}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5ByVb-zMVL-",
        "outputId": "26b6e223-fc03-413d-f6c9-b4adccda9e67"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no: 26%\n",
            "yes: 74%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 전혀 관련 없는 문장을 target_sequence로 설정하여 실행\n",
        "\n",
        "target_sequence = 'The boy quickly ran across the finish line, seizing yet another victory'\n",
        "tokens = tokenizer(input_sentence, target_sequence, return_tensors='pt') \n",
        "logits = model(**tokens).logits \n",
        "results = torch.softmax(logits, dim=1).tolist()[0] \n",
        "\n",
        "for i, label in enumerate(['no', 'yes']):\n",
        "  print(f\"{label}:{int(round(results[i]*100))}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRDj9woObCBB",
        "outputId": "9e477fd1-d1eb-4ef7-9761-d818b6a749c6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no:95%\n",
            "yes:5%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 감성분석에 BERT 이용하기\n",
        "\n",
        "import nltk \n",
        "from nltk.corpus import movie_reviews\n",
        "from sklearn.model_selection import train_test_split \n",
        "import numpy as np\n",
        "\n",
        "nltk.download('movie_reviews') \n",
        "\n",
        "# movie review data에서 file id를 가져옴 \n",
        "fileids = movie_reviews.fileids() \n",
        "\n",
        "# file id를 이용하여 raw text file 가져옴\n",
        "reviews = [movie_reviews.raw(fileids) for fileid in fileids] \n",
        "categories = [movie_reviews.categories(fileid)[0] for fileid in fileids] \n",
        "\n",
        "# label을 0,1의 값으로 변환\n",
        "label_dict = {'pos':1, 'neg':0} \n",
        "y = np.array([label_dict[c] for c in categories]) \n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(reviews, y, test_size=0.2, random_state=7) \n",
        "print('Train set count:', len(X_train)) \n",
        "print('Test set count:', len(X_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8Nys4sO_gBm",
        "outputId": "fa4fdd4b-855b-4c9d-fd72-e88cbc6b6ebe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification \n",
        "import torch\n",
        "import torch.nn.functional as F  \n",
        "\n",
        "# cuda를 이용한 GPU연산이 가능하다면 cuda를 사용하고, 아니면 cpu를 사용 \n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') \n",
        "\n",
        "# Auto Classes를 이용하여 사전학습된 내용에 맞는 토크나이저와 모형을 자동으로 설정 \n",
        "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english') \n",
        "model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english') \n",
        "\n",
        "# 모델을 gpu로 옮겨서 연산 준비\n",
        "model = model.to(device) \n",
        "\n",
        "# 모형으로 한번에 예측할 데이터의 수\n",
        "batch_size = 10\n",
        "\n",
        "# 전체 예측결과를 저장 \n",
        "y_pred = [] \n",
        "\n",
        "num_batch = len(y_test)//batch_size \n",
        "\n",
        "for i in range(num_batch):\n",
        "  inputs = tokenizer(X_test[i*batch_size:(i+1)*batch_size], truncation=True, padding=True, return_tensors='pt') \n",
        "\n",
        "  # 토큰화 결과를 GPU로 이동\n",
        "  inputs = inputs.to(device) \n",
        "\n",
        "  # 모형으로 결과를 예측\n",
        "  logits = model(**inputs).logits \n",
        "\n",
        "  # 결과값을 클래스에 대한 확률로 변환 \n",
        "  pred = F.softmax(logits, dim=-1) \n",
        "\n",
        "  # 예측결과를 CPU로 가져와서 넘파이로 변환한 후, argmax로 확률이 가장 큰 클래스를 선택함 \n",
        "  results = pred.cpu().detach().numpy().argmax(axis=1) \n",
        "\n",
        "  # 전체 예측결과에 추가 \n",
        "  y_pred.extend(results.tolist()) \n",
        "\n",
        "\n",
        "# gpu 메모리를 비움\n",
        "torch.cuda.empty_cache() \n",
        "\n",
        "score = sum(y_test == np.array(y_pred)/len(y_test)) \n",
        "print('NLTK 영화리뷰 감성분석 정확도:', score)"
      ],
      "metadata": {
        "id": "2SCw7J90C9mM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K5JwowM0Gkdq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}