{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMBddSLejRian1Ut3te4MQI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmalicekim/NLP_practice_mine/blob/master/5.%20BOW_%EA%B8%B0%EB%B0%98%EC%9D%98_%EB%AC%B8%EC%84%9C%EB%B6%84%EB%A5%98.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-e6_MnKppjy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BOW 기반의 문서 분류\n",
        "\n",
        "## 1. 20뉴스그룹 데이터 준비 및 특성 추출"
      ],
      "metadata": {
        "id": "fjIkL73UqYrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# 20개의 토픽 중 선택하고자 하는 토픽을 리스트로 생성\n",
        "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
        "\n",
        "# 학습 데이터셋 가져옴\n",
        "newsgroups_train = fetch_20newsgroups(subset = 'train', \n",
        "                                      remove = ('headers', 'footers', 'quotes'), #메일 내용에서 hint가 되는 부분 삭제 (순수하게 내용만으로 분류하기위해)\n",
        "                                      categories = categories)\n",
        "\n",
        "# 검증 데이터셋 가져옴\n",
        "newsgroups_test = fetch_20newsgroups(subset = 'test', \n",
        "                                     remove = ('headers', 'footers', 'quotes'),\n",
        "                                     categories = categories)\n",
        "\n",
        "print('#Train set size:', len(newsgroups_train.data))\n",
        "print('#Test set size:', len(newsgroups_test.data))\n",
        "print('#Selected categories:', newsgroups_train.target_names)\n",
        "print('#Train labels:', set(newsgroups_train.target))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrdjZ5qqqe27",
        "outputId": "12571231-97bb-48bf-b4e5-8226a0202564"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Train set size: 2034\n",
            "#Test set size: 1353\n",
            "#Selected categories: ['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']\n",
            "#Train labels: {0, 1, 2, 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('#Train set text samples:', newsgroups_train.data[0])\n",
        "print('#Train set label samples:', newsgroups_train.target[0])\n",
        "print('#Test set text samples:', newsgroups_test.data[0])\n",
        "print('#Test set label samples:', newsgroups_test.target[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDD87ovar4B3",
        "outputId": "3c2d91cc-cea0-411e-807e-704a95abaf87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Train set text samples: Hi,\n",
            "\n",
            "I've noticed that if you only save a model (with all your mapping planes\n",
            "positioned carefully) to a .3DS file that when you reload it after restarting\n",
            "3DS, they are given a default position and orientation.  But if you save\n",
            "to a .PRJ file their positions/orientation are preserved.  Does anyone\n",
            "know why this information is not stored in the .3DS file?  Nothing is\n",
            "explicitly said in the manual about saving texture rules in the .PRJ file. \n",
            "I'd like to be able to read the texture rule information, does anyone have \n",
            "the format for the .PRJ file?\n",
            "\n",
            "Is the .CEL file format available from somewhere?\n",
            "\n",
            "Rych\n",
            "#Train set label samples: 1\n",
            "#Test set text samples: TRry the SKywatch project in  Arizona.\n",
            "#Test set label samples: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = newsgroups_train.data #학습 데이터셋 문서\n",
        "y_train = newsgroups_train.target #학습 데이터셋 라벨\n",
        "\n",
        "X_test = newsgroups_test.data #검증 데이터셋 문서\n",
        "y_test = newsgroups_test.target #검증 데이터셋 라벨\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "cv = CountVectorizer(max_features=2000, min_df=5, max_df=0.5) #5이상 & 전체의 50% 안넘어가는 것만\n",
        "\n",
        "X_train_cv = cv.fit_transform(X_train) #train set을 변환\n",
        "print('Train set dimension:', X_train_cv.shape)\n",
        "\n",
        "X_test_cv = cv.fit_transform(X_test) #test set을 변환\n",
        "print('Test set dimension:', X_test_cv.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSXYlQERshJm",
        "outputId": "d9ca4e33-12b8-4b36-9bf6-8dfecd881f13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set dimension: (2034, 2000)\n",
            "Test set dimension: (1353, 2000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(cv.get_feature_names()))\n",
        "\n",
        "for word, count in zip(cv.get_feature_names_out()[:100], X_train_cv[0].toarray()[0,:100]):\n",
        "  print(word, ':', count, end=', ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sj6lepJTt8F_",
        "outputId": "b3783304-e7ee-4603-8da1-c4724c326379"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2000\n",
            "00 : 0, 000 : 0, 10 : 0, 100 : 0, 11 : 0, 12 : 0, 128 : 0, 129 : 0, 13 : 0, 130 : 0, 14 : 0, 15 : 0, 150 : 0, 16 : 0, 17 : 0, 18 : 0, 19 : 0, 192 : 0, 1987 : 0, 1988 : 0, 1989 : 0, 1990 : 0, 1991 : 0, 1992 : 0, 1993 : 0, 20 : 0, 200 : 0, 21 : 0, 22 : 0, 23 : 0, 24 : 0, 24bit : 0, 25 : 0, 256 : 0, 26 : 0, 27 : 0, 275 : 0, 28 : 0, 286 : 0, 29 : 0, 2d : 0, 30 : 0, 300 : 0, 31 : 0, 32 : 0, 33 : 0, 34 : 0, 35 : 0, 36 : 0, 386 : 0, 3d : 0, 40 : 0, 41 : 0, 42 : 0, 44 : 0, 45 : 0, 48 : 0, 50 : 0, 500 : 0, 60 : 0, 61 : 0, 64 : 0, 65 : 0, 70 : 0, 75 : 0, 80 : 0, 800 : 0, 90 : 0, 91 : 0, 92 : 0, 93 : 1, 95 : 0, ability : 1, able : 0, abortion : 0, about : 0, above : 0, absolute : 0, absolutely : 0, ac : 0, accept : 0, accepted : 0, access : 0, according : 0, account : 0, across : 0, act : 0, action : 0, actions : 0, active : 0, activity : 0, actual : 0, actually : 0, ad : 0, add : 0, added : 0, addition : 0, additional : 0, address : 0, addresses : 0, "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 머신러닝과 문서 분류 과정에 대한 이해\n",
        "\n",
        "## 3. 나이브 베이즈 분류기 (Naive Bayse Classifier)를 이용한 문서 분류"
      ],
      "metadata": {
        "id": "cs17e666w1Qn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB #sklearn이 제공하는 MultinomialNB 사용\n",
        "NB_clf = MultinomialNB() # 분류기 선언\n",
        "\n",
        "NB_clf.fit(X_train_cv, y_train) #train set을 이용하여 분류기(classifier)를 학습\n",
        "\n",
        "print('Train set score: {:.3f}'.format(NB_clf.score(X_train_cv, y_train))) #train set에 대한 예측정확도 확인\n",
        "print('Test set score: {:.3f}'.format(NB_clf.score(X_test_cv, y_test))) #test set에 대한 예측정확도 확인"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVx8Clomt_ea",
        "outputId": "6b0ebb0a-f26a-4be8-b2d3-3f72741c5dcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set score: 0.824\n",
            "Test set score: 0.368\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('#First document and label in test data:', X_test[0], y_test[0])\n",
        "print('#Second document and label in test data:', X_test[1], y_test[1])\n",
        "\n",
        "pred = NB_clf.predict(X_test_cv[:2]) \n",
        "\n",
        "print('#Predicted labels:', pred)\n",
        "print('#Predicted categories:', newsgroups_train.target_names[pred[0]], newsgroups_train.target_names[pred[1]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Riyv0JvyYzG",
        "outputId": "d3a3287d-8c2c-4baf-9a17-7640450f8af8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#First document and label in test data: TRry the SKywatch project in  Arizona. 2\n",
            "#Second document and label in test data: The Vatican library recently made a tour of the US.\n",
            " Can anyone help me in finding a FTP site where this collection is \n",
            " available. 1\n",
            "#Predicted labels: [1 2]\n",
            "#Predicted categories: comp.graphics sci.space\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "#CountVectorizer와 동일한 인수 사용\n",
        "tfidf = TfidfVectorizer(max_features=2000, min_df=5, max_df=0.5)\n",
        "X_train_tfidf = tfidf.fit_transform(X_train) #train set을 변환\n",
        "X_test_tfidf = tfidf.transform(X_test) #test set을 변환\n",
        "\n",
        "NB_clf.fit(X_train_tfidf, y_train)\n",
        "print('Train set score: {:.3f}'.format(NB_clf.score(X_train_tfidf, y_train)))\n",
        "print('Test set score: {:.3f}'.format(NB_clf.score(X_test_tfidf, y_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsbZDaax_-Pb",
        "outputId": "7cdc7ff7-2f40-46a1-9b8d-b8d9b97cfe6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set score: 0.862\n",
            "Test set score: 0.741\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def top10_features(classifier, vectorizer, categories):\n",
        "  feature_names = np.asarray(vectorizer.get_feature_names_out())  # asarray() : convert an given input to an array\n",
        "  for i, category in enumerate(categories):\n",
        "    # 역순으로 정렬하기 위해 계수에 음수를 취해서 정렬 후 앞에서부터 10개의 값을 반환\n",
        "    top10 = np.argsort(-classifier.coef_[i])[:10]\n",
        "\n",
        "    # 카테고리와 영향이 큰 특성 10개 출력\n",
        "    print(\"{}:{}\".format(category, \", \".join(feature_names[top10])))\n",
        "\n",
        "top10_features(NB_clf, tfidf, newsgroups_train.target_names)    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gf5pX4HqBU69",
        "outputId": "4b44b863-a045-4a68-a2f8-fd888a36cabd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "alt.atheism:you, not, are, be, this, have, as, what, they, if\n",
            "comp.graphics:you, on, graphics, this, have, any, can, or, with, thanks\n",
            "sci.space:space, on, you, be, was, this, as, they, have, are\n",
            "talk.religion.misc:you, not, he, are, as, this, be, god, was, they\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. 로지스틱 회귀분석을 이용한 문서 분류"
      ],
      "metadata": {
        "id": "Mn2Pc6JeI10_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression \n",
        "\n",
        "# count vector에 대해 regression을 해서 NB와 비교\n",
        "LR_clf = LogisticRegression() # 분류기 선언\n",
        "LR_clf.fit(X_train_tfidf, y_train)\n",
        "print('Train set score: {:.3f}'.format(LR_clf.score(X_train_tfidf, y_train)))\n",
        "print('Test set score: {:.3f}'.format(LR_clf.score(X_test_tfidf, y_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9Mxw_ChEINE",
        "outputId": "2effc1ef-d5b5-44aa-99c6-e4053c51c8e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set score: 0.930\n",
            "Test set score: 0.734\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 릿지 회귀 (ridge regression)"
      ],
      "metadata": {
        "id": "nKGsU2clJkHY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import RidgeClassifier\n",
        "\n",
        "ridge_clf = RidgeClassifier()\n",
        "ridge_clf.fit(X_train_tfidf, y_train)\n",
        "print('Train set score: {:.3f}'.format(ridge_clf.score(X_train_tfidf, y_train)))\n",
        "print('Test set score: {:.3f}'.format(ridge_clf.score(X_test_tfidf, y_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmsT_AeiJW2i",
        "outputId": "32c885fe-53a9-4203-fa95-1b4bb4c9b51d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set score: 0.960\n",
            "Test set score: 0.735\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split \n",
        "\n",
        "X_train_ridge, X_val_ridge, y_train_ridge, y_val_ridge = train_test_split(\n",
        "    X_train_tfidf, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "max_score = 0\n",
        "max_alpha = 0\n",
        "\n",
        "# ridge에서 가중치(w)의 절댓값을 가능한 한 작게 만드려고 함. 과대적합이 되지 않도록 모델을 강제로 제한(L2규제)\n",
        "# alpha 매개변수로 훈련 세트의 성능 대비 모델을 얼마나 단순화할지를 지정 가능 (기본값 alpha = 1.0)\n",
        "# alpha 값을 높이면 계수를 0에 더 가깝게 만들어 훈련세트의 성능은 나빠지지만 일반화에는 쉬워짐\n",
        "\n",
        "for alpha in np.arange(0.1, 10, 0.1): #alpha를 0.1부터 10까지 0.1씩 증가 \n",
        "  ridge_clf = RidgeClassifier(alpha=alpha)\n",
        "  ridge_clf.fit(X_train_ridge, y_train_ridge) \n",
        "  score = ridge_clf.score(X_val_ridge, y_val_ridge)\n",
        "  if score > max_score:\n",
        "    max_score = score\n",
        "    max_alpha = alpha\n",
        "\n",
        "print('Max alpha {:.3f} at max validation score {:.3f}'.format(max_alpha, max_score))    \n"
      ],
      "metadata": {
        "id": "cBD6MMQGKEJA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22568e2f-fcd4-4adc-c44c-bc4dea192aae"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max alpha 1.600 at max validation score 0.826\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ridge_clf = RidgeClassifier(alpha=1.6)\n",
        "ridge_clf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "print('Train set score: {:.3f}'.format(ridge_clf.score(X_train_tfidf, y_train)))\n",
        "print('Test set score: {:.3f}'.format(ridge_clf.score(X_test_tfidf, y_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjgeRG5tPKKy",
        "outputId": "d24039e6-0a95-4db2-e9aa-ceaee201ce05"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set score: 0.948\n",
            "Test set score: 0.739\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top10_features(ridge_clf, tfidf, newsgroups_train.target_names) #ridge 분류기가 훨씬 좋은 결과물 보여줌"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WLSPybPPhtq",
        "outputId": "364b8a5f-8cb1-4d8b-c2a6-9ac5736ada52"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "alt.atheism:bobby, religion, atheism, atheists, motto, punishment, islam, deletion, islamic, satan\n",
            "comp.graphics:graphics, computer, 3d, file, image, hi, 42, using, screen, looking\n",
            "sci.space:space, orbit, nasa, spacecraft, moon, sci, launch, flight, funding, idea\n",
            "talk.religion.misc:christian, christians, fbi, blood, order, jesus, objective, children, christ, hudson\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "ACWOtOICP7no"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 라쏘 회귀분석(Lasso regression)을 이용한 특성 선택 (feature selection)"
      ],
      "metadata": {
        "id": "9ejhi9wZPzWu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lasso_clf = LogisticRegression(penalty='l1', solver='liblinear', C=1) #Lasso는 동일한 LogisticRegression을 사용하면서 매개변수로 지정. penalty='l1'으로 해주어야 함\n",
        "lasso_clf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "print('#Train set score: {:.3f}'.format(lasso_clf.score(X_train_tfidf, y_train)))\n",
        "print('#Test set score: {:.3f}'.format(lasso_clf.score(X_test_tfidf, y_test)))\n",
        "\n",
        "\n",
        "# 계수(coefficient)중에서 0이 아닌 것들의 개수 출력\n",
        "print('#Used features count: {}'.format(np.sum(lasso_clf.coef_ !=0)), 'out of', X_train_tfidf.shape[1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XClNOR8P5Lp",
        "outputId": "43488df9-778f-473f-a7e1-70da3590d147"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Train set score: 0.819\n",
            "#Test set score: 0.724\n",
            "#Used features count: 437 out of 2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top10_features(lasso_clf, tfidf, newsgroups_train.target_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iEJDUfSZI4O",
        "outputId": "39040c74-1006-4e8b-b585-6c6321b1ade7"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "alt.atheism:bobby, atheism, atheists, islam, religion, islamic, motto, atheist, satan, vice\n",
            "comp.graphics:graphics, image, 3d, file, computer, hi, video, files, looking, sphere\n",
            "sci.space:space, orbit, launch, nasa, spacecraft, flight, moon, dc, shuttle, solar\n",
            "talk.religion.misc:fbi, christian, christians, christ, order, jesus, children, objective, context, blood\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "x91oFbYWeZ2R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. 결정트리 등을 이용한 기타 문서 분류 방법"
      ],
      "metadata": {
        "id": "J1lMrrXIeWwB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "tree = DecisionTreeClassifier(random_state = 7)\n",
        "tree.fit(X_train_tfidf, y_train)\n",
        "print('#Decision Tree train set score: {:.3f}'.format(tree.score(X_train_tfidf, y_train)))\n",
        "print('#Decision Tree test set score: {:.3f}'.format(tree.score(X_test_tfidf, y_test)))\n",
        "\n",
        "forest = RandomForestClassifier(random_state = 7)\n",
        "forest.fit(X_train_tfidf, y_train)\n",
        "print('#Decision Tree train set score: {:.3f}'.format(forest.score(X_train_tfidf, y_train)))\n",
        "print('#Decision Tree test set score: {:.3f}'.format(forest.score(X_test_tfidf, y_test)))\n",
        "\n",
        "gb = GradientBoostingClassifier(random_state = 7)\n",
        "gb.fit(X_train_tfidf, y_train)\n",
        "print('#Decision Tree train set score: {:.3f}'.format(gb.score(X_train_tfidf, y_train)))\n",
        "print('#Decision Tree test set score: {:.3f}'.format(gb.score(X_test_tfidf, y_test)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8O-arKzzeZQi",
        "outputId": "4236dfb1-8cd5-4ca4-eb3b-45abbae05a2e"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Decision Tree train set score: 0.977\n",
            "#Decision Tree test set score: 0.536\n",
            "#Decision Tree train set score: 0.977\n",
            "#Decision Tree test set score: 0.685\n",
            "#Decision Tree train set score: 0.933\n",
            "#Decision Tree test set score: 0.696\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sort & lambda 예제\n",
        "data_list = ['but','i','wont','hesitate','no','more','no','more','it','cannot','wait','im','yours']\n",
        "\n",
        "data_list = list(set(data_list))\n",
        "\n",
        "data_list.sort()\n",
        "print(data_list) \n",
        "\n",
        "data_list.sort(key=lambda x:len(x))\n",
        "print(data_list)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "173toWCYfgrr",
        "outputId": "93c940a9-06a1-4842-b1b0-2b05cad52525"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['but', 'cannot', 'hesitate', 'i', 'im', 'it', 'more', 'no', 'wait', 'wont', 'yours']\n",
            "['i', 'im', 'it', 'no', 'but', 'more', 'wait', 'wont', 'yours', 'cannot', 'hesitate']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_feature_importances = sorted(zip(tfidf.get_feature_names_out(), gb.feature_importances_), \n",
        "                                    key=lambda x:x[1], reverse=True)\n",
        "\n",
        "for feature, value in sorted_feature_importances[:40]:\n",
        "  print('{}:{}'.format(feature, value), end=', ')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdEWhMlOfH9D",
        "outputId": "e01728df-3aea-4915-c585-3cb9ff9f1d2b"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "space:0.12606815556503398, graphics:0.07955738535484687, atheism:0.023543258791466362, thanks:0.023046507462732756, file:0.02055581240238831, orbit:0.020063273938036058, jesus:0.01814149212443511, god:0.017709491540890448, hi:0.016857094927865728, nasa:0.015308260768876465, image:0.015023452519852164, files:0.013797145686816017, christ:0.010426583770790022, moon:0.010376173054679735, bobby:0.010225361352506715, launch:0.010061136447910934, looking:0.009733618292917431, christian:0.009663039306452306, atheists:0.00940203835982551, christians:0.008981046753380314, fbi:0.008827368636033589, 3d:0.007819020701620837, you:0.007751791923437367, not:0.007535276098728061, islamic:0.007255118998822582, religion:0.007182617149762632, spacecraft:0.007074189152295397, flight:0.006899060277543797, computer:0.006698870635101267, islam:0.006529364772733121, ftp:0.0062457492372051055, color:0.005513872126424359, software:0.004918770389689076, atheist:0.004805134811116732, card:0.004767940213268275, people:0.004624503816321023, koresh:0.004608372640219624, his:0.004571604182502271, kent:0.004415620817426218, sphere:0.004334578233902467, "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "VvmgFBTSl1DN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. 성능을 높이기 위한 방법들"
      ],
      "metadata": {
        "id": "5gXXj5wolyBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 library들을 import\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "cachedStopWords = stopwords.words(\"english\")\n",
        "\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import re\n",
        "\n",
        "RegTok = RegexpTokenizer(\"[\\w']{3,}\") #정규표현식으로 토크나이저 정의\n",
        "english_stops = set(stopwords.words('english')) #영어 불용어를 가져옴\n",
        "\n",
        "def tokenizer(text):\n",
        "  tokens = RegTok.tokenize(text.lower())\n",
        "  words = [word for word in tokens if (word not in english_stops) and len(word) > 2 ]\n",
        "  features = (list(map(lambda token : PorterStemmer().stem(token), words)))\n",
        "  return features\n",
        "\n",
        "tfidf = TfidfVectorizer(tokenizer=tokenizer, max_features=2000, min_df=5, max_df=0.5)\n",
        "X_train_tfidf = tfidf.fit_transform(X_train) #train set을 변환\n",
        "X_test_tfidf = tfidf.transform(X_test) # test set을 변환 --- test set은 fit_transform 이 아니라 transform 만!\n",
        "\n",
        "#tfidf vector를 이용해서 분류기 학습\n",
        "LR_clf = LogisticRegression()\n",
        "LR_clf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "print('#Train set score: {:.3f}'.format(LR_clf.score(X_train_tfidf, y_train)))\n",
        "print('#Test set score: {:.3f}'.format(LR_clf.score(X_test_tfidf, y_test)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLTTIeqqj9O5",
        "outputId": "3cc284b3-a38d-47bd-de33-1976d6624229"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Train set score: 0.930\n",
            "#Test set score: 0.751\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(LR_clf.coef_[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0CIvz4enL0f",
        "outputId": "4a799ba9-e3e0-4232-9021-eeec54a34e42"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2000"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer(tokenizer=tokenizer)\n",
        "\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "print('#Train set dimension:', X_train_tfidf.shape)\n",
        "X_test_tfidf = tfidf.transform(X_test)\n",
        "print('#Test set dimension:', X_test_tfidf.shape)\n",
        "\n",
        "ridge_clf = RidgeClassifier(alpha=2.4)\n",
        "ridge_clf.fit(X_train_tfidf, y_train)\n",
        "print('#Train set score:{:.3f}'.format(ridge_clf.score(X_train_tfidf, y_train)))\n",
        "print('#Test set score:{:.3f}'.format(ridge_clf.score(X_test_tfidf, y_test)))\n",
        "\n",
        "NB_clf = MultinomialNB(alpha=0.01)\n",
        "NB_clf.fit(X_train_tfidf, y_train)\n",
        "print('#Train set score: {:.3f}'.format(NB_clf.score(X_train_tfidf, y_train)))\n",
        "print('#Test set score: {:.3f}'.format(NB_clf.score(X_test_tfidf, y_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6tnWQuUvdv6",
        "outputId": "65ab4f3b-3a64-481b-abdd-1cc356142f4c"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Train set dimension: (2034, 20085)\n",
            "#Test set dimension: (1353, 20085)\n",
            "#Train set score:0.968\n",
            "#Test set score:0.768\n",
            "#Train set score: 0.971\n",
            "#Test set score: 0.793\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "7KcWSGfTyhyO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. 카운트 기반의 문제점과 N-gram"
      ],
      "metadata": {
        "id": "AZ4NiNthyc7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cachedStopWords = stopwords.words(\"english\")\n",
        "tfidf = TfidfVectorizer(token_pattern=\"[a-zA-Z]{3,}\", #토큰화를 위한 정규식\n",
        "                        decode_error='ignore',\n",
        "                        lowercase=True,\n",
        "                        stop_words=stopwords.words('english'),\n",
        "                        max_df=0.5, min_df=2)\n",
        "\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf.transform(X_test)\n",
        "\n",
        "print(X_train_tfidf.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qmKbEOKyfPb",
        "outputId": "753b48fd-0edb-4d68-9450-8e6cb2f9ccd6"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2034, 11346)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import RidgeClassifier\n",
        "\n",
        "ridge_clf = RidgeClassifier()\n",
        "ridge_clf.fit(X_train_tfidf, y_train) #학습\n",
        "print('Train set score: {:.3f}'.format(ridge_clf.score(X_train_tfidf, y_train)))\n",
        "print('Test set score: {:.3f}'.format(ridge_clf.score(X_test_tfidf, y_test)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5vMi4NizHZ7",
        "outputId": "6c2ad6a2-67d4-43f6-9573-794fac323bf4"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set score: 0.976\n",
            "Test set score: 0.769\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# bigram으로 설정하기\n",
        "\n",
        "tfidf = TfidfVectorizer(token_pattern=\"[a-zA-Z]{3,}\", \n",
        "                        decode_error='ignore',\n",
        "                        lowercase=True,\n",
        "                        stop_words=stopwords.words('english'),\n",
        "                        ngram_range=(1,2), #바이그램 설정\n",
        "                        max_df=0.5, min_df=2)\n",
        "\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf.transform(X_test)\n",
        "\n",
        "print(X_train_tfidf.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f56obNflzgoz",
        "outputId": "f22d8ecb-fddd-410e-cbb0-ee2ace29a0e1"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2034, 26348)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_features = [f for f in tfidf.get_feature_names_out() if len(f.split()) > 1]\n",
        "\n",
        "print('bi-gram samples:', bigram_features[:10])\n",
        "\n",
        "ridge_clf.fit(X_train_tfidf, y_train) #학습\n",
        "print('Train set score: {:.3f}'.format(ridge_clf.score(X_train_tfidf, y_train)))\n",
        "print('Test set score: {:.3f}'.format(ridge_clf.score(X_test_tfidf, y_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rv2NsZ550G6Y",
        "outputId": "78951130-795b-4aa1-c13d-8a906d596c93"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bi-gram samples: ['aas american', 'ability means', 'ability pass', 'able accept', 'able afford', 'able control', 'able convince', 'able draw', 'able establish', 'able find']\n",
            "Train set score: 0.976\n",
            "Test set score: 0.776\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# trigram으로 설정하기\n",
        "\n",
        "tfidf = TfidfVectorizer(token_pattern= \"[a-zA-Z']{3,}\", \n",
        "                        decode_error ='ignore', \n",
        "                        lowercase=True, \n",
        "                        stop_words = stopwords.words('english'),\n",
        "                        ngram_range=(1, 3),\n",
        "                        max_df=0.5,\n",
        "                        min_df=2)\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf.transform(X_test)\n",
        "print(X_train_tfidf.shape)\n",
        "\n",
        "trigram_features = [f for f in tfidf.get_feature_names_out() if len(f.split()) > 2]\n",
        "print('tri-gram samples:', trigram_features[:10])\n",
        "\n",
        "ridge_clf.fit(X_train_tfidf, y_train) #학습\n",
        "print('Train set score: {:.3f}'.format(ridge_clf.score(X_train_tfidf, y_train)))\n",
        "print('Test set score: {:.3f}'.format(ridge_clf.score(X_test_tfidf, y_test)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgScF5Ll0956",
        "outputId": "08c7944c-f113-43d8-a1d3-3d2994adac75"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2034, 32943)\n",
            "tri-gram samples: [\"'em better shots\", \"'expected errors' basically\", \"'karla' next one\", \"'nodis' password also\", \"'official doctrine think\", \"'ok see warning\", \"'what's moonbase good\", 'aas american astronautical', 'ability means infallible', 'able accept donations']\n",
            "Train set score: 0.976\n",
            "Test set score: 0.775\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "8KIYaXZQ2qcT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. 한글 문서의 분류"
      ],
      "metadata": {
        "id": "-hEviuaH2r9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('./daum_movie_review.csv')\n",
        "df.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "yWughblp2rVp",
        "outputId": "a9d5ca54-1bc9-47a3-8063-a67dca16ac04"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-90-383fd42277e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./daum_movie_review.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './daum_movie_review.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uT36lVOy21mh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}